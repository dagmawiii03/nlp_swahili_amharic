{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a80245",
   "metadata": {},
   "source": [
    "# Modelling and Deployment using MLOps \n",
    "\n",
    "Now that we have audio input data & corresponding labels in an array format, it is easier to consume and apply Natural language processing techniques. We can convert audio files labels into integers using label Encoding or One Hot Vector Encoding for machines to learn. The labeled dataset will help us in the neural network model output layer for predicting results. These help in training & validation datasets into nD array.\n",
    "At this stage, we apply other pre-processing techniques like dropping columns, normalization, etc. to conclude our final training data for building models. Moving to the next stage of splitting the dataset into train, test, and validation is what we have been doing for other models. \n",
    "We can leverage CNN, RNN, LSTM,CTC etc. deep neural algorithms to build and train the models for speech applications like speech recognition. The model trained with the standard size few seconds audio chunk transformed into an array of n dimensions with the respective labels will result in predicting output labels for test audio input. As output labels will vary beyond binary, we are talking about building a multi-class label classification method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75aaa220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] (size =27)\n",
      "C:\\Users\\Sebli\\Desktop\\10x Files\\Week 4\\nlp_swahili_amharic\\notebooks\n",
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "import tensorflow as tf\n",
    "from clean import Clean\n",
    "from utils import vocab\n",
    "from deep_learner import DeepLearn\n",
    "from modeling import Modeler\n",
    "from evaluator import CallbackEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b39c85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78298269",
   "metadata": {},
   "outputs": [],
   "source": [
    "AM_ALPHABET='ሀለሐመሠረሰቀበግዕዝተኀነአከወዐዘየደገጠጰጸፀፈፐቈኈጐኰፙፘፚauiāeəo'\n",
    "EN_ALPHABET='abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d2ac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 09:04:36,224:logger:Successfully initialized clean class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n"
     ]
    }
   ],
   "source": [
    "cleaner = Clean()\n",
    "char_to_num,num_to_char=vocab(AM_ALPHABET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f81812",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880d08c",
   "metadata": {},
   "source": [
    "**objective**: Build a Deep learning model that converts speech to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f636c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_df = pd.read_csv('../data/swahili.csv')\n",
    "lang = pd.read_csv(\"../data/swahili.csv\")\n",
    "lang['type']='swahili'\n",
    "amharic_df = pd.read_csv(\"../data/amharic.csv\")\n",
    "amharic_df['type']='amharic'\n",
    "language_df = lang.append(amharic_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe74d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = Modeler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f37929",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_preprocessed = pre_model.preprocessing_learn(swahili_df,'key','file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a83b3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "amharic_preprocessed = pre_model.preprocessing_learn(amharic_df,'key','file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c00fc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,val_df,test_df = amharic_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee6ecf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "# Define the trainig dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(train_df[\"file\"]), list(train_df[\"text\"]))\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.map(cleaner.encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Define the validation dataset\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(val_df[\"file\"]), list(val_df[\"text\"]))\n",
    ")\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(cleaner.encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0baf182",
   "metadata": {},
   "source": [
    "## Deep Learnin Architecture - CNN - RNN - LSTM & CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db5ee6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepSpeech_2\"\n",
      "______________________________________________________________________________________________________________\n",
      " Layer (type)                                    Output Shape                                Param #          \n",
      "==============================================================================================================\n",
      " input (InputLayer)                              [(None, None, 2)]                           0                \n",
      "                                                                                                              \n",
      " expand_dim (Reshape)                            (None, None, 2, 1)                          0                \n",
      "                                                                                                              \n",
      " conv_1 (Conv2D)                                 (None, None, 1, 2)                          4                \n",
      "                                                                                                              \n",
      " conv_1_bn (BatchNormalization)                  (None, None, 1, 2)                          8                \n",
      "                                                                                                              \n",
      " conv_1_relu (ReLU)                              (None, None, 1, 2)                          0                \n",
      "                                                                                                              \n",
      " conv_2 (Conv2D)                                 (None, None, 1, 2)                          4                \n",
      "                                                                                                              \n",
      " conv_2_bn (BatchNormalization)                  (None, None, 1, 2)                          8                \n",
      "                                                                                                              \n",
      " conv_2_relu (ReLU)                              (None, None, 1, 2)                          0                \n",
      "                                                                                                              \n",
      " reshape (Reshape)                               (None, None, 2)                             0                \n",
      "                                                                                                              \n",
      " bidirectional_1 (Bidirectional)                 (None, None, 2)                             30               \n",
      "                                                                                                              \n",
      " dense_1 (Dense)                                 (None, None, 2)                             6                \n",
      "                                                                                                              \n",
      " dense_1_relu (ReLU)                             (None, None, 2)                             0                \n",
      "                                                                                                              \n",
      " dropout (Dropout)                               (None, None, 2)                             0                \n",
      "                                                                                                              \n",
      " dense (Dense)                                   (None, None, 45)                            135              \n",
      "                                                                                                              \n",
      "==============================================================================================================\n",
      "Total params: 195\n",
      "Trainable params: 187\n",
      "Non-trainable params: 8\n",
      "______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learn = DeepLearn(input_width=1, label_width=1, shift=1,epochs=5,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                 label_columns=['mfcc-0'])\n",
    "fft_length = 2\n",
    "model = learn.build_asr_model(\n",
    "    input_dim=fft_length // 2 + 1,\n",
    "    output_dim=char_to_num.vocabulary_size(),\n",
    "    rnn_units=1,\n",
    ")\n",
    "model.summary(line_length=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87fcda",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244b50a3",
   "metadata": {},
   "source": [
    "**objective**: Evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9b59de5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: nan  The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n",
      "1/1 [==============================] - 1s 521ms/step\n",
      "The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n",
      "1/1 [==============================] - 1s 630ms/step\n",
      "The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n",
      "1/1 [==============================] - 1s 978ms/step\n",
      "The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "one or more groundtruths are empty strings",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m validation_callback \u001b[38;5;241m=\u001b[39m CallbackEval(model,validation_dataset)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mvalidation_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Desktop\\10x Files\\Week 4\\nlp_swahili_amharic\\scripts\\evaluator.py:33\u001b[0m, in \u001b[0;36mCallbackEval.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     29\u001b[0m         label \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     30\u001b[0m             tf\u001b[38;5;241m.\u001b[39mstrings\u001b[38;5;241m.\u001b[39mreduce_join(num_to_char(label))\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     32\u001b[0m         targets\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m---> 33\u001b[0m wer_score \u001b[38;5;241m=\u001b[39m \u001b[43mwer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord Error Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwer_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\jiwer\\measures.py:72\u001b[0m, in \u001b[0;36mwer\u001b[1;34m(truth, hypothesis, truth_transform, hypothesis_transform, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwer\u001b[39m(\n\u001b[0;32m     58\u001b[0m     truth: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m     59\u001b[0m     hypothesis: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     63\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    Calculate word error rate (WER) between a set of ground-truth sentences and\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    a set of hypothesis sentences.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :return: WER as a floating point number\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     measures \u001b[38;5;241m=\u001b[39m compute_measures(\n\u001b[0;32m     73\u001b[0m         truth, hypothesis, truth_transform, hypothesis_transform, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     74\u001b[0m     )\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m measures[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\jiwer\\measures.py:206\u001b[0m, in \u001b[0;36mcompute_measures\u001b[1;34m(truth, hypothesis, truth_transform, hypothesis_transform, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     hypothesis \u001b[38;5;241m=\u001b[39m [hypothesis]\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m truth):\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone or more groundtruths are empty strings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Preprocess truth and hypothesis\u001b[39;00m\n\u001b[0;32m    209\u001b[0m truth, hypothesis \u001b[38;5;241m=\u001b[39m _preprocess(\n\u001b[0;32m    210\u001b[0m     truth, hypothesis, truth_transform, hypothesis_transform\n\u001b[0;32m    211\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: one or more groundtruths are empty strings"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "# Callback function to check transcription on the val set.\n",
    "validation_callback = CallbackEval(model,validation_dataset)\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[validation_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bee2bc",
   "metadata": {},
   "source": [
    "## Model Space Exploration\n",
    "Using hyperparameter optimization by slightly modifying the architecture e.g. increasing and decreasing the number of layers to find the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5a981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CONV_RNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " the_input (InputLayer)      [(None, None, 128)]       0         \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, None, 128, 1)      0         \n",
      "                                                                 \n",
      " cnn_0 (Conv2D)              (None, None, 122, 16)     800       \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, None, 122, 16)     0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, None, 60, 16)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " bn_cnn_0 (BatchNormalizatio  (None, None, 60, 16)     64        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " cnn_1 (Conv2D)              (None, None, 56, 32)      12832     \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, None, 56, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, None, 27, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " bn_cnn_1 (BatchNormalizatio  (None, None, 27, 32)     128       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " cnn_2 (Conv2D)              (None, None, 25, 64)      18496     \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, None, 25, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, None, 12, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " bn_cnn_2 (BatchNormalizatio  (None, None, 12, 64)     256       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, None, 768)         0         \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, None, 1024)       3938304   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bn_rnn_0 (BatchNormalizatio  (None, None, 1024)       4096      \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, None, 1024)       4724736   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bn_rnn_1 (BatchNormalizatio  (None, None, 1024)       4096      \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, None, 1024)       4724736   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bn_rnn_2 (BatchNormalizatio  (None, None, 1024)       4096      \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, None, 1024)       4724736   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bn_rnn_3 (BatchNormalizatio  (None, None, 1024)       4096      \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, None, 224)        229600    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, None, 224)         0         \n",
      "                                                                 \n",
      " dense_1_relu (LeakyReLU)    (None, None, 224)         0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, None, 224)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,391,072\n",
      "Trainable params: 18,382,656\n",
      "Non-trainable params: 8,416\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "filters = [16, 32, 64]\n",
    "kernels = [7, 5, 3] \n",
    "pool_sizes = [3, 3, 3]  \n",
    "mx_stride = [1, 1, 2]\n",
    "cnn_stride = 1 \n",
    "input_dim = 128\n",
    "\n",
    "CONV_RNN_Model = learn.build_model(input_dim, filters, kernels, pool_sizes, mx_stride, cnn_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d8ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
