{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9924fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing essential modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a0e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing local modules\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "\n",
    "from clean import Clean\n",
    "from utils import vocab\n",
    "from deep_learner import DeepLearn\n",
    "from modeling import Modeler\n",
    "from evaluator import CallbackEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8487bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping an object\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ad00a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AM_ALPHABET='ሀለሐመሠረሰቀበግዕዝተኀነአከወዐዘየደገጠጰጸፀፈፐቈኈጐኰፙፘፚauiāeəo'\n",
    "EN_ALPHABET='abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6aa9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-06 19:38:32,213:logger:Successfully initialized clean class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n"
     ]
    }
   ],
   "source": [
    "#create cleaner object to numerate every amharic alphabets\n",
    "cleaner = Clean()\n",
    "char_to_num,num_to_char=vocab(AM_ALPHABET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f51888",
   "metadata": {},
   "source": [
    "# Deep Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2425776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv of each dataset into their respective dataframes\n",
    "swahili_df = pd.read_csv('../data/swahili.csv')\n",
    "lang = pd.read_csv(\"../data/swahili.csv\")\n",
    "lang['type']='swahili'\n",
    "\n",
    "amharic_df = pd.read_csv(\"../data/amharic.csv\")\n",
    "amharic_df['type']='amharic'\n",
    "language_df = lang.append(amharic_df, ignore_index=True) #used to append two datasets and combine them in to a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c605c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of modeler class\n",
    "pre_model = Modeler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84cfbae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating pre processed swahilli for deep learning\n",
    "swahili_preprocessed = pre_model.preprocessing_learn(swahili_df,'key','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d58f8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating pre processed amharic for deep learning\n",
    "amharic_preprocessed = pre_model.preprocessing_learn(amharic_df,'key','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b6e4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate amharic pre processed into train, test, validation dataset\n",
    "train_df,val_df,test_df = amharic_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b849609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "# Define the trainig dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(train_df[\"file\"]), list(train_df[\"text\"]))\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.map(cleaner.encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Define the validation dataset\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(val_df[\"file\"]), list(val_df[\"text\"]))\n",
    ")\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(cleaner.encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec57b1",
   "metadata": {},
   "source": [
    "## Deep Learning Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d6a8d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepSpeech_2\"\n",
      "______________________________________________________________________________________________________________\n",
      " Layer (type)                                    Output Shape                                Param #          \n",
      "==============================================================================================================\n",
      " input (InputLayer)                              [(None, None, 2)]                           0                \n",
      "                                                                                                              \n",
      " expand_dim (Reshape)                            (None, None, 2, 1)                          0                \n",
      "                                                                                                              \n",
      " conv_1 (Conv2D)                                 (None, None, 1, 2)                          4                \n",
      "                                                                                                              \n",
      " conv_1_bn (BatchNormalization)                  (None, None, 1, 2)                          8                \n",
      "                                                                                                              \n",
      " conv_1_relu (ReLU)                              (None, None, 1, 2)                          0                \n",
      "                                                                                                              \n",
      " conv_2 (Conv2D)                                 (None, None, 1, 2)                          4                \n",
      "                                                                                                              \n",
      " conv_2_bn (BatchNormalization)                  (None, None, 1, 2)                          8                \n",
      "                                                                                                              \n",
      " conv_2_relu (ReLU)                              (None, None, 1, 2)                          0                \n",
      "                                                                                                              \n",
      " reshape (Reshape)                               (None, None, 2)                             0                \n",
      "                                                                                                              \n",
      " bidirectional_1 (Bidirectional)                 (None, None, 2)                             30               \n",
      "                                                                                                              \n",
      " dense_1 (Dense)                                 (None, None, 2)                             6                \n",
      "                                                                                                              \n",
      " dense_1_relu (ReLU)                             (None, None, 2)                             0                \n",
      "                                                                                                              \n",
      " dropout (Dropout)                               (None, None, 2)                             0                \n",
      "                                                                                                              \n",
      " dense (Dense)                                   (None, None, 45)                            135              \n",
      "                                                                                                              \n",
      "==============================================================================================================\n",
      "Total params: 195\n",
      "Trainable params: 187\n",
      "Non-trainable params: 8\n",
      "______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learn = DeepLearn(input_width=1, label_width=1, shift=1,epochs=5,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                 label_columns=['mfcc-0'])\n",
    "fft_length = 2\n",
    "model = learn.build_asr_model(\n",
    "    input_dim=fft_length // 2 + 1,\n",
    "    output_dim=char_to_num.vocabulary_size(),\n",
    "    rnn_units=1,\n",
    ")\n",
    "model.summary(line_length=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02574934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
